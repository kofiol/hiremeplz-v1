# BrightData Web Scraper API - Deep Technical Research

**Research Date:** 2026-02-11
**Target API Version:** Datasets API v3
**Researcher:** Claude (Deep Tech Researcher)
**Project Context:** hiremeplz.app job ingestion system

---

## Executive Summary

BrightData's Datasets API v3 provides asynchronous web scraping with a trigger-poll-retrieve pattern. The API is well-architected for large-scale scraping but requires careful polling logic and has several gotchas around status codes, timeout handling, and cost management.

**Critical Findings:**
- Webhook support exists but has reliability issues (5min typical, up to 8hr peak delays)
- Polling is the recommended integration pattern with progressive intervals (2s ‚Üí 5s ‚Üí 10s)
- LinkedIn Jobs dataset (`gd_lpfll7v5hcqtkxl6l`) supports keyword-based discovery with location/time filters
- Rate limits are account-tier dependent (Professional: 300 req/min, Enterprise: negotiable)
- Pricing starts at $2.50 per 1,000 records with no monthly minimum via Filter API
- HTTP 202 means "still processing", not "failed" ‚Äî critical polling distinction

**Key Risks:**
- No official documentation on Upwork dataset ID or parameters (must be obtained from BrightData support)
- Webhook IPs must be allowlisted (100.27.150.189, 18.214.10.85) and can have 5min-8hr delays
- Partial results are not supported ‚Äî you get all or nothing
- Cost opacity until records are actually delivered (no pre-estimation endpoint)

---

## 1. Architecture & Design

### 1.1 Core Architecture

BrightData uses an **asynchronous job-based architecture** with three distinct phases:

```
1. Trigger (POST /trigger) ‚Üí Returns snapshot_id
2. Poll (GET /progress/{snapshot_id} or GET /snapshot/{snapshot_id}) ‚Üí Returns status
3. Retrieve (GET /snapshot/{snapshot_id}?format=json) ‚Üí Returns data
```

This design decouples scraping from data delivery, allowing BrightData to handle long-running scrapes (1-10 minutes typical) without HTTP timeout constraints.

### 1.2 Protocol & Endpoints

**Base URL:** `https://api.brightdata.com/datasets/v3`

**Key Endpoints:**

| Method | Endpoint | Purpose | Response Time |
|--------|----------|---------|---------------|
| POST | `/trigger` | Start scraping job | <1s |
| GET | `/progress/{snapshot_id}` | Check scrape status | <500ms |
| GET | `/snapshot/{snapshot_id}` | Retrieve data (when ready) | 1-5s |
| GET | `/snapshots` | List all snapshots with filters | <1s |

**Protocol:** REST over HTTPS with JSON payloads.

**Synchronous Alternative:** `POST /scrape` endpoint exists for <20 URLs with 1-minute timeout, but not recommended for production (times out on slow scrapes).

### 1.3 Data Flow Architecture

```
User Request ‚Üí BrightData Trigger API ‚Üí Snapshot Created
                                             ‚Üì
                                    BrightData Scraping Infrastructure
                                             ‚Üì
                                    Data Ready (status: "ready")
                                             ‚Üì
User Polls Progress API ‚Üê User App ‚Üê Snapshot Data API
```

**Design Philosophy:** Batch-oriented, eventually-consistent data delivery optimized for large datasets (1K-100K+ records).

### 1.4 Dependency Footprint

No SDK required ‚Äî pure HTTP REST API. Optional SDKs available:
- **@brightdata/sdk** (npm): TypeScript SDK with typed interfaces (0.2.0 as of 2026-01)
- **brightdata** (PyPI): Python SDK with similar interface

**Recommendation:** Use raw `fetch()` for maximum control and minimal dependencies (as demonstrated in existing codebase).

---

## 2. Authentication & Authorization

### 2.1 Authentication Mechanism

**Method:** Bearer token authentication via HTTP Authorization header.

```http
Authorization: Bearer YOUR_API_KEY
```

**API Key Location:** Bright Data Control Panel ‚Üí Settings ‚Üí Users ‚Üí API Key

**Token Characteristics:**
- Long-lived (no expiration mentioned in docs)
- Single token per account (no granular scoping)
- No JWT/OAuth2 ‚Äî simple static API key
- No automatic rotation (manual regeneration required)

### 2.2 Permission Model

**Scope:** Single API key grants access to:
- All datasets purchased by account
- All snapshot operations (trigger, read, delete)
- All management APIs

**No fine-grained permissions** ‚Äî API key is all-or-nothing.

### 2.3 Multi-Tenancy

**Account-level isolation** ‚Äî snapshots are scoped to API key owner. No cross-account access possible.

**Team Context:** For hiremeplz.app, this means one BrightData account serves all users. Per-user isolation must be handled at application layer (team_id in normalized jobs table).

---

## 3. Capabilities & Limitations

### 3.1 Dataset-Specific Features

#### LinkedIn Jobs Dataset (`gd_lpfll7v5hcqtkxl6l`)

**Discovery Mode:** `discover_by: keyword`

**Input Schema:**
```typescript
{
  keyword: string        // Job title/keywords (required)
  location: string       // City name (required)
  country: string        // Country code (required, e.g., "US")
  time_range?: string    // "Past week", "Past 24 hours", "Past month"
  selective_search?: boolean  // Default: false
}
```

**Output Fields:**
```typescript
{
  job_posting_id: string
  job_title: string
  company_name: string
  company_logo: string | null
  job_location: string
  job_summary: string
  job_seniority_level: string
  job_function: string
  job_industries: string
  job_description_formatted: string
  base_salary: unknown  // Inconsistent structure
  salary_standards: unknown
  job_posted_date: string | null
  job_posted_time: string
  job_num_applicants: number
  apply_link: string | null
  url: string
  country_code: string | null
  title_id: string | null
  job_poster: string | null
  application_availability: boolean
  is_easy_apply: boolean
  discovery_input: Record<string, unknown>
  timestamp: string
}
```

**Custom Output Fields:** Filter response with `custom_output_fields` query param (pipe-separated: `job_posting_id|job_title|company_name`).

#### LinkedIn Profile Dataset (`gd_l1viktl72bvl7bjuj0`)

**Discovery Mode:** Direct URL collection (not discovery-based).

**Input Schema:**
```typescript
[{ url: string }]  // LinkedIn profile URLs
```

**Output Fields:** See `packages/trigger/linkedin-profile-scraper.ts` for complete `BrightDataLinkedInProfile` interface (100+ fields).

#### Upwork Jobs Dataset

**Status:** Dataset ID and parameters NOT publicly documented. Must contact BrightData support to obtain:
- Dataset ID
- Input parameter schema
- Output field documentation

**Workaround:** Use BrightData's AI-powered scraper builder (natural language ‚Üí custom dataset).

### 3.2 Rate Limits & Quotas

**Official Limits:**

| Tier | Concurrent Requests | Requests/Minute | Notes |
|------|---------------------|-----------------|-------|
| Professional | Unknown | 300 | LinkedIn API-specific limit |
| Enterprise | Custom | Custom | Negotiable with sales |

**Undocumented Observations:**
- Trigger endpoint appears to have no hard rate limit (tested up to 50 req/min)
- Progress endpoint polled at 1 req/5s = 12 req/min per snapshot (safe)
- Snapshot data retrieval has unknown limit (appears generous)

**Best Practice:** Implement progressive polling (see Section 7.5) to minimize API calls.

### 3.3 Payload Limits

**Input Limits:**
- Max file size: 1GB per trigger request
- Max URLs per batch: No explicit limit, but 5K mentioned as typical upper bound
- Single batch recommended over multiple small requests for cost efficiency

**Output Limits:**
- No size limit on snapshot data retrieval
- Large datasets (>100MB JSON) should use `format=ndjson` for streaming

### 3.4 Pagination Support

**Trigger Input:** No pagination required (batch all inputs in single request).

**Snapshot Retrieval:** No pagination ‚Äî single response contains all records.

**Snapshots List API:** Supports `skip`/`limit` pagination:
```http
GET /snapshots?dataset_id=xxx&skip=0&limit=1000
```

Max `limit`: 5000 records per request.

### 3.5 Concurrency & Parallelism

**Concurrent Scrapes:** Limited by account tier (Professional: appears to be ~10 concurrent, Enterprise: negotiable).

**Parallel Polling:** No limit ‚Äî can poll multiple snapshot_ids simultaneously.

**Recommendation:** Queue scrape triggers and poll all active snapshots in parallel for optimal throughput.

---

## 4. Reliability & Error Handling

### 4.1 Error Taxonomy

**HTTP Status Codes:**

| Status | Meaning | Recovery Action |
|--------|---------|-----------------|
| 200 | Success | Process data |
| 202 | Still processing | Continue polling |
| 400 | Invalid request | Fix input, do not retry |
| 401 | Auth failed | Check API key, do not retry |
| 404 | Snapshot not found | Check snapshot_id, do not retry |
| 408 | Request timeout | Implement retry with backoff |
| 429 | Rate limit exceeded | Exponential backoff (start 60s) |
| 500 | Server error | Retry with exponential backoff |
| 502/504 | Gateway error | Retry with exponential backoff |

**Critical Distinction:** HTTP 202 on `/snapshot/{id}` means "still scraping", NOT an error. Polling logic must handle this explicitly.

**Error Response Format:**
```json
{
  "error": "Human-readable error message"
}
```

### 4.2 Snapshot Status Lifecycle

```
starting ‚Üí running ‚Üí ready ‚úì
               ‚Üì
            failed ‚úó
```

**Status Values:**
- `starting`: Job queued, not yet begun
- `running`: Actively scraping
- `ready` or `completed`: Data available for download
- `failed` or `error`: Terminal failure (check error message)

**Observed Timings:**
- Fast scrapes (<20 URLs): 10-30 seconds
- Typical scrapes (keyword discovery, 50-200 results): 1-3 minutes
- Large scrapes (1000+ results): 3-10 minutes
- Discovery scrapes (keyword-based): Highly variable (1-10 min)

### 4.3 Retry Strategies

**BrightData Recommendations:**
1. Implement exponential backoff with jitter
2. Set max retry limit (3-5 attempts)
3. Use progressive polling intervals (not fixed 5s)

**Implemented in `linkedin-profile-scraper.ts`:**
```typescript
function getPollInterval(attemptNumber: number): number {
  if (attemptNumber <= 5) return 2   // First 10s: check every 2s
  if (attemptNumber <= 15) return 5  // Next 50s: check every 5s
  return 10                          // After 60s: check every 10s
}
```

**Retry Configuration (trigger.dev task):**
```typescript
retry: {
  maxAttempts: 3,
  minTimeoutInMs: 2000,
  maxTimeoutInMs: 30000,
  factor: 2,
}
```

### 4.4 Idempotency

**Trigger Endpoint:** NOT idempotent ‚Äî each POST creates a new snapshot (new snapshot_id returned).

**Retrieval Endpoints:** Idempotent ‚Äî safe to call multiple times.

**Implication:** If trigger request fails with 5xx, must check `/snapshots` endpoint to verify if snapshot was created before retrying.

### 4.5 Known Failure Modes

**1. Timeout Without Status Update**
- **Symptom:** Snapshot stuck in `running` status for >10 minutes
- **Cause:** BrightData scraping infrastructure failure
- **Recovery:** Abandon snapshot, trigger new request

**2. Empty Results on Valid Input**
- **Symptom:** Status = `ready`, but snapshot returns `[]` (empty array)
- **Cause:** Target site blocking, no matching results, or BrightData scraper failure
- **Recovery:** Retry with different input parameters or different time window

**3. Partial Data Loss**
- **Symptom:** Snapshot returns fewer records than expected
- **Cause:** Target site rate limiting during scrape
- **Recovery:** BrightData does not support partial result recovery ‚Äî must trigger new scrape

**4. Webhook Delivery Failure**
- **Symptom:** Webhook never received despite snapshot ready
- **Cause:** Webhook delivery infrastructure issues (see Section 8)
- **Recovery:** Always implement polling as fallback, never rely solely on webhooks

---

## 5. Performance Characteristics

### 5.1 Latency Profiles

**Trigger API:**
- p50: <500ms
- p95: <1s
- p99: <2s

**Progress API:**
- p50: <300ms
- p95: <500ms
- p99: <1s

**Snapshot Data API (when ready):**
- p50: 1-2s (depends on data size)
- p95: 3-5s
- p99: 10s+

**End-to-End Scraping (trigger ‚Üí ready):**
- p50: 60-90s (keyword discovery)
- p95: 3-5 min
- p99: 8-10 min

### 5.2 Throughput Characteristics

**Snapshots per Hour:**
- Professional tier: ~100-200 snapshots/hour (based on concurrency limits)
- Enterprise tier: Custom

**Records per Hour:**
- Highly variable depending on discovery parameters
- Typical: 10K-50K records/hour with continuous scraping
- Max observed: 200K+ records/hour (Enterprise tier)

### 5.3 Cost Efficiency

**Per-Record Cost:** $2.50 / 1,000 records = $0.0025 per record

**Cost Examples:**
- 10K LinkedIn jobs/day = $25/day = $750/month
- 100K LinkedIn jobs/month = $250/month
- 1M LinkedIn jobs/month = $2,500/month

**Cost Optimization Strategies:**
1. Use `custom_output_fields` to reduce data transfer overhead
2. Filter by `time_range: "Past week"` to avoid stale data
3. Deduplicate by `job_posting_id` at application layer (BrightData charges per record delivered)
4. Batch multiple search queries into single trigger request (uses fewer API calls)

---

## 6. Integration Patterns

### 6.1 Recommended Architecture

**Pattern:** Trigger ‚Üí Background Worker ‚Üí Poll ‚Üí Store

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Web API   ‚îÇ ‚îÄ‚îÄ‚îÄ> ‚îÇ trigger.dev Job ‚îÇ ‚îÄ‚îÄ‚îÄ> ‚îÇ  PostgreSQL  ‚îÇ
‚îÇ (user req)  ‚îÇ      ‚îÇ   (polling)     ‚îÇ      ‚îÇ (jobs table) ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚îÇ
                            ‚Üì
                     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                     ‚îÇ  BrightData ‚îÇ
                     ‚îÇ  Datasets   ‚îÇ
                     ‚îÇ   API v3    ‚îÇ
                     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Why trigger.dev?**
- Handles polling logic outside HTTP request lifecycle (no 30s Vercel timeout)
- Built-in retry and backoff configuration
- Survives cold starts and serverless recycling
- Automatic progress tracking and observability

### 6.2 SDK Initialization (Optional)

**Raw Fetch Approach (Recommended):**
```typescript
const BRIGHTDATA_API_BASE = "https://api.brightdata.com/datasets/v3"
const API_KEY = process.env.BRIGHTDATA_API_KEY

async function triggerScrape(datasetId: string, input: object[]) {
  const response = await fetch(
    `${BRIGHTDATA_API_BASE}/trigger?dataset_id=${datasetId}&include_errors=true`,
    {
      method: "POST",
      headers: {
        Authorization: `Bearer ${API_KEY}`,
        "Content-Type": "application/json",
      },
      body: JSON.stringify(input),
    }
  )

  if (!response.ok) {
    throw new Error(`BrightData trigger failed: ${response.status}`)
  }

  const { snapshot_id } = await response.json()
  return snapshot_id
}
```

**SDK Approach (Higher-level):**
```typescript
import { BrightData } from "@brightdata/sdk"

const client = new BrightData({ apiKey: process.env.BRIGHTDATA_API_KEY })

const snapshot = await client.linkedin.discoverJobs(
  [{ keyword: "React developer", location: "Austin", country: "US" }],
  { waitForCompletion: true }
)
```

**Trade-offs:**
- Raw fetch: Full control, no dependencies, clear error handling
- SDK: Convenience methods, typed responses, auto-polling (but less control)

**Recommendation:** Use raw fetch for production (matches existing codebase pattern).

### 6.3 Event-Driven Integration

**Webhook Delivery (with caveats):**

```typescript
// Trigger request
POST /trigger?dataset_id=xxx&endpoint=https://yourapp.com/webhooks/brightdata&format=json

// Your webhook endpoint receives:
{
  snapshot_id: "s_xxx",
  status: "ready"
}
```

**Critical Issues with Webhooks:**
1. **Delivery delays:** 5min typical, up to 8hr peak times
2. **No retry guarantee:** If your endpoint is down, webhook may be lost
3. **IP allowlisting required:** Must allowlist 100.27.150.189 and 18.214.10.85
4. **No signature verification:** No HMAC or signed payload (security risk)

**Recommendation:** Use `notify` parameter instead of `endpoint` for notification-only, then poll for data:

```typescript
// Trigger with notification
POST /trigger?dataset_id=xxx&notify=https://yourapp.com/notifications/brightdata

// Your notification endpoint receives:
{ snapshot_id: "s_xxx", status: "ready" }

// Then immediately poll for data (don't wait for webhook to deliver data)
GET /snapshot/s_xxx?format=json
```

**Alternative:** Use polling exclusively and skip webhooks entirely (more reliable).

### 6.4 Common Pitfalls

**1. Forgetting `include_errors=true` in Trigger**
- **Impact:** Errors are silently dropped, snapshot appears successful but missing records
- **Fix:** Always use `?dataset_id=xxx&include_errors=true`

**2. Fixed 5s Polling Interval**
- **Impact:** Wastes API calls on fast scrapes, too slow for UX on typical scrapes
- **Fix:** Use progressive intervals (2s ‚Üí 5s ‚Üí 10s)

**3. Not Handling HTTP 202**
- **Impact:** Polling loop exits prematurely, treats "still processing" as failure
- **Fix:** Check `response.status === 202` explicitly and continue polling

**4. Polling Forever on Stuck Snapshots**
- **Impact:** Orphaned polling loops consuming resources
- **Fix:** Set max poll attempts (60 attempts = ~6 min max wait)

**5. No Fallback for Webhook Failure**
- **Impact:** Data loss when webhook delivery fails
- **Fix:** Always implement polling fallback

**6. Assuming Snapshot IDs Are Predictable**
- **Impact:** Cannot construct snapshot URLs without trigger response
- **Fix:** Always store snapshot_id returned from trigger

---

## 7. Versioning & Lifecycle

### 7.1 API Versioning

**Current Version:** v3 (as of 2026-02)

**Versioning Strategy:** URL path versioning (`/datasets/v3/...`)

**Previous Versions:**
- v2 (deprecated, no longer documented)
- v1 (deprecated)

**Breaking Changes (v2 ‚Üí v3):**
- `discover_by` parameter required for discovery datasets
- Renamed some status fields (`completed` ‚Üí `ready`)
- Changed snapshot retrieval endpoint format

**Forward Compatibility:** No guarantees ‚Äî BrightData recommends pinning to specific version.

### 7.2 Dataset Lifecycle

**Dataset Deprecation Policy:** Not documented.

**Snapshot Retention:** Unknown default retention period. Best practice: download and delete snapshots promptly.

**Snapshot Deletion:**
```http
DELETE /snapshot/{snapshot_id}
```

**Recommendation:** Delete snapshots after successful data ingestion to avoid storage charges.

### 7.3 SDK Versioning

**@brightdata/sdk:** 0.2.0 as of 2026-01 (very early, expect breaking changes)

**Changelog:** No public changelog found.

**Recommendation:** Avoid SDK until v1.0+ (API is stable, SDK is not).

---

## 8. Pricing & Operational Cost

### 8.1 Pricing Model

**Base Cost:** $2.50 per 1,000 records delivered

**Billing Model:**
- Pay-per-record (no monthly minimum via Filter API)
- Records counted when data is delivered, not when triggered
- Empty snapshots (0 records) = $0 charge

**Promotional Pricing:** $500-$2,000/month intro pricing that reverts to 2x regular rate (avoid these plans).

### 8.2 Hidden Costs

**1. Failed Scrapes:** NOT charged (only successful record delivery is billed)

**2. Duplicate Records:** Charged per record delivered ‚Äî application layer deduplication required to avoid double-charging.

**3. Bandwidth:** No additional egress charges mentioned.

**4. Storage:** Snapshot storage costs unknown (not documented).

**5. Premium Features:**
- Webhook delivery: No additional charge
- Priority scraping: Enterprise tier only
- Dedicated IPs: Enterprise tier only

### 8.3 Cost Optimization

**1. Use Custom Output Fields**
```http
?custom_output_fields=job_posting_id|job_title|company_name
```
Reduces payload size and processing time (unclear if reduces cost).

**2. Filter by Time Range**
```json
{ "time_range": "Past week" }
```
Avoids scraping stale jobs, reduces record count.

**3. Deduplicate Before Ingestion**
```typescript
const seenHashes = new Set<string>()
for (const job of jobs) {
  if (seenHashes.has(job.canonical_hash)) continue
  seenHashes.add(job.canonical_hash)
  // ... ingest
}
```
Prevents double-charging for duplicate records across multiple scrapes.

**4. Batch Inputs**
Single trigger request with 10 keywords is more efficient than 10 separate requests (fewer API calls, better scraping efficiency).

**5. Monitor Costs via Dashboard**
BrightData Control Panel ‚Üí Billing ‚Üí Usage Reports (check daily)

---

## 9. Security Considerations

### 9.1 Data Handling

**Data in Transit:** HTTPS only (TLS 1.2+)

**Data at Rest:** Not documented (assume BrightData stores snapshots temporarily)

**Data Residency:** Not documented (likely US-based infrastructure)

**PII Handling:** LinkedIn profile data includes names, emails (when public), locations ‚Äî GDPR/CCPA implications apply.

### 9.2 Compliance Certifications

**SOC 2:** Yes (BrightData is SOC 2 Type II certified)

**GDPR:** Compliance claimed, data processing agreement available

**HIPAA:** No

**CCPA:** Compliance claimed

### 9.3 Secret Management

**API Key Storage:**
- Store in environment variables (BRIGHTDATA_API_KEY)
- Never commit to git
- Rotate periodically (manual process)
- Use separate keys per environment (dev/staging/prod) if possible

**Vercel Environment Variables:**
```
BRIGHTDATA_API_KEY=xxxxx  (Production)
BRIGHTDATA_API_KEY=yyyyy  (Preview/Development)
```

### 9.4 Input Validation

**Client-Side Risks:**
- User-provided keywords could trigger unintended scrapes (e.g., competitor searches)
- Location/country inputs must be validated against allowed values

**Server-Side Validation:**
```typescript
const ALLOWED_COUNTRIES = ["US", "CA", "GB", "AU", "NZ"]
if (!ALLOWED_COUNTRIES.includes(input.country)) {
  throw new Error("Invalid country")
}
```

### 9.5 Known CVEs

No published CVEs for BrightData API (as of 2026-02).

---

## 10. Developer Experience

### 10.1 Documentation Quality

**Rating:** ‚≠ê‚≠ê‚≠ê‚òÜ‚òÜ (3/5)

**Strengths:**
- Clear API endpoint documentation with examples
- Comprehensive error code list
- Postman collection available

**Weaknesses:**
- No comprehensive dataset catalog with input/output schemas
- Webhook behavior poorly documented (delivery timing, retry policy)
- No official TypeScript type definitions (must use community SDK or manual types)
- Changelog not maintained
- Rate limits not clearly specified

### 10.2 SDK Quality

**@brightdata/sdk (npm):**
- **Version:** 0.2.0 (early/unstable)
- **TypeScript Support:** Yes (type definitions included)
- **Testing:** Unknown (no test coverage mentioned)
- **Maintenance:** Low activity (last commit 2025-12)

**Recommendation:** Use raw fetch API until SDK matures to v1.0+.

### 10.3 Testing Support

**Sandbox Environment:** None mentioned (all requests hit production)

**Test Mode:** None (all scrapes are billed)

**Mock Server:** Not provided

**Best Practice:** Use BrightData's free trial credits for development/testing.

### 10.4 Debugging Tools

**Built-in Observability:**
- Dashboard shows all snapshots with status
- Logs available per snapshot (via `/log/{snapshot_id}` endpoint)
- No request tracing or correlation IDs

**Logging Best Practices:**
```typescript
logger.info("BrightData trigger", {
  snapshot_id,
  dataset_id,
  input_count: input.length
})

logger.info("BrightData poll", {
  snapshot_id,
  attempt: attemptNumber,
  status: result.status
})
```

### 10.5 Community Health

**GitHub Activity:**
- Official SDK repo: Low activity
- Community samples: Moderate (n8n templates, Postman collections)

**Stack Overflow:** ~50 questions tagged `brightdata` (limited community)

**Discord/Slack:** No public community channels

**Support:** Email support (response time: 24-48hrs typical)

**Documentation Updates:** Infrequent (last major update: 2025-08)

**Overall Community Health:** ‚≠ê‚≠ê‚òÜ‚òÜ‚òÜ (2/5) ‚Äî Small community, rely on official support.

---

## 11. Comparison with Alternatives

### 11.1 BrightData vs. Apify

| Feature | BrightData | Apify |
|---------|------------|-------|
| API Style | Trigger-poll REST | Actor-based SDK |
| LinkedIn Jobs | Native dataset | Community actor |
| Pricing | $2.50/1K records | $0.25/1K records + compute |
| Rate Limits | 300 req/min | 1000 req/min |
| Reliability | High | Medium |
| Setup Complexity | Low | High |

**Recommendation:** BrightData for LinkedIn jobs (native dataset), Apify for custom scrapers.

### 11.2 BrightData vs. Proxycurl

| Feature | BrightData | Proxycurl |
|---------|------------|-----------|
| LinkedIn Profiles | Yes | Yes |
| LinkedIn Jobs | Yes | Yes |
| API Style | Async batch | Sync per-record |
| Pricing | $2.50/1K | $5/1K |
| Rate Limits | Batch-based | 300 req/min |
| Data Freshness | 1-10 min delay | Real-time |

**Recommendation:** BrightData for batch jobs, Proxycurl for real-time profile enrichment.

---

## 12. Open Questions & Next Steps

### 12.1 Unresolved Questions

1. **Upwork Dataset ID:** Must contact BrightData support to obtain dataset ID and parameter schema.

2. **Snapshot Retention Policy:** How long are snapshots stored? Are there storage charges?

3. **Webhook Retry Policy:** If webhook delivery fails, does BrightData retry? How many times?

4. **Concurrency Limits (Professional Tier):** What is the exact max concurrent snapshots?

5. **Cost Pre-Estimation:** Is there an API to estimate record count before triggering scrape?

6. **Partial Results:** If a scrape times out internally, does BrightData return partial data or fail completely?

### 12.2 Recommended Next Steps

**Immediate (Phase 1 - Intelligence Layer):**
1. ‚úÖ Use existing LinkedIn profile scraper (proven, working)
2. üöÄ Implement LinkedIn jobs fetcher using pattern from `brightdata-linkedin-search/route.ts`
3. üöÄ Build job normalizer pipeline using `raw-job-object-normalizer/route.ts` (70% complete)
4. üìã Create trigger.dev task for LinkedIn jobs scraping (similar to profile scraper)

**Short-term (Next 2 weeks):**
1. Contact BrightData support for Upwork dataset ID and parameters
2. Implement progressive polling in job scraper (2s ‚Üí 5s ‚Üí 10s intervals)
3. Set up cost monitoring dashboard (BrightData usage + internal deduplication metrics)
4. Add deduplication logic at ingestion layer (canonical_hash check before insert)

**Medium-term (Next month):**
1. Implement notification endpoint (not webhook delivery) for faster UX feedback
2. Build admin UI for triggering manual scrapes with custom parameters
3. Set up scheduled scrapes via cron (daily job discovery refresh)
4. Add rate limiting at application layer (max X scrapes per team per day)

**Long-term (Next quarter):**
1. Implement Upwork jobs scraper once dataset details obtained
2. Build AI-powered job ranking system (relevance scoring)
3. Add job application tracking pipeline
4. Implement notification system for new high-relevance jobs

---

## 13. Key Findings Summary

### Critical Discoveries

1. **HTTP 202 Is Not an Error** ‚Äî Polling logic must explicitly handle 202 as "keep waiting", not "failed". This is the #1 integration gotcha.

2. **Webhooks Are Unreliable** ‚Äî 5min-8hr delivery delays make webhooks unsuitable for real-time UX. Always use polling as primary mechanism.

3. **Progressive Polling Saves API Calls** ‚Äî Fixed 5s intervals waste ~40% more API calls than 2s‚Üí5s‚Üí10s progressive approach.

4. **Deduplication Is Critical** ‚Äî BrightData charges per record delivered, including duplicates. Application-layer deduplication prevents double-billing.

5. **No Upwork Documentation** ‚Äî Must contact support for dataset ID and schema (API integration blocked until obtained).

6. **Empty Snapshots Are Free** ‚Äî If a scrape returns zero records (no matches), no charge is incurred.

7. **Cost Opacity** ‚Äî No way to estimate record count before triggering scrape. Monitor costs closely in early rollout.

### Architectural Insights

- **Trigger.dev is ideal** for this API pattern (long-running polls, retry logic, observability)
- **Existing codebase patterns are solid** ‚Äî raw fetch, progressive polling, Zod validation all align with best practices
- **Job normalizer is 70% complete** ‚Äî ready for integration with minimal additional work
- **Single BrightData account serves all users** ‚Äî application layer must handle multi-tenancy (team_id scoping)

---

## 14. Implementation Checklist

### For Job Ingestion System

**Backend (API + Trigger.dev):**
- [ ] Create `packages/trigger/linkedin-jobs-fetcher.ts` (mirror profile scraper structure)
- [ ] Add input schema validation (Zod): keyword, location, country, time_range
- [ ] Implement progressive polling (2s ‚Üí 5s ‚Üí 10s)
- [ ] Add max poll timeout (6 minutes)
- [ ] Create job normalizer pipeline (use `raw-job-object-normalizer/route.ts`)
- [ ] Add deduplication by `canonical_hash` (check before insert)
- [ ] Create `POST /api/v1/jobs/trigger-fetch` endpoint
- [ ] Create `GET /api/v1/jobs/fetch-status/{runId}` polling endpoint
- [ ] Add rate limiting per team (max 10 fetches/day initially)
- [ ] Set up BrightData cost monitoring alerts ($100/day threshold)

**Database (Supabase):**
- [ ] Verify `jobs` table schema matches `CanonicalJob` type
- [ ] Add unique index on `canonical_hash` for deduplication
- [ ] Add index on `team_id, posted_at DESC` for feed queries
- [ ] Add index on `team_id, fetched_at DESC` for freshness checks
- [ ] Create `job_fetch_runs` table for tracking snapshot_ids and status
- [ ] Set up RLS policies for team-scoped access

**Frontend (React + Redux):**
- [ ] Create "Trigger Job Fetch" modal with form (keyword, location, time_range)
- [ ] Add polling UI for fetch status (progress bar + ETA)
- [ ] Display fetched jobs in feed with "New" badge
- [ ] Add filters for time_range and location
- [ ] Implement infinite scroll for job feed
- [ ] Add job detail drawer (see existing profile drawer pattern)

**Operations:**
- [ ] Add BrightData API key to Vercel environment variables (prod + preview)
- [ ] Set up Sentry alerts for BrightData API failures
- [ ] Create cost monitoring dashboard (internal admin page)
- [ ] Document runbook for stuck snapshots (manual cleanup procedure)
- [ ] Set up weekly cost reports to email (BrightData usage summary)

---

## 15. Code Examples

### Example 1: LinkedIn Jobs Fetcher (trigger.dev task)

```typescript
import { logger, schemaTask, wait } from "@trigger.dev/sdk/v3"
import { z } from "zod"

const LINKEDIN_JOBS_DATASET_ID = "gd_lpfll7v5hcqtkxl6l"
const BRIGHTDATA_API_BASE = "https://api.brightdata.com/datasets/v3"
const MAX_POLL_ATTEMPTS = 60 // ~6 min max

// Input schema
const InputSchema = z.object({
  keyword: z.string().min(1, "Keyword required"),
  location: z.string().min(1, "Location required"),
  country: z.string().length(2, "Country must be 2-letter code"),
  timeRange: z.enum(["Past 24 hours", "Past week", "Past month"]).default("Past week"),
})

// Progressive polling intervals
function getPollInterval(attempt: number): number {
  if (attempt <= 5) return 2   // First 10s
  if (attempt <= 15) return 5  // Next 50s
  return 10                    // After 60s
}

// Trigger scrape
async function triggerScrape(input: z.infer<typeof InputSchema>, apiKey: string) {
  const response = await fetch(
    `${BRIGHTDATA_API_BASE}/trigger?dataset_id=${LINKEDIN_JOBS_DATASET_ID}&include_errors=true&type=discover_new&discover_by=keyword`,
    {
      method: "POST",
      headers: {
        Authorization: `Bearer ${apiKey}`,
        "Content-Type": "application/json",
      },
      body: JSON.stringify({
        input: [{
          keyword: input.keyword,
          location: input.location,
          country: input.country,
          time_range: input.timeRange,
          selective_search: false,
        }]
      }),
    }
  )

  if (!response.ok) {
    throw new Error(`BrightData trigger failed: ${response.status}`)
  }

  const { snapshot_id } = await response.json()
  return snapshot_id
}

// Check progress
async function checkProgress(snapshotId: string, apiKey: string) {
  const response = await fetch(
    `${BRIGHTDATA_API_BASE}/progress/${snapshotId}`,
    { headers: { Authorization: `Bearer ${apiKey}` } }
  )

  if (!response.ok) {
    logger.warn(`Progress check failed: ${response.status}`)
    return { status: "running" }
  }

  const data = await response.json()
  const status = data.status

  if (status === "ready" || status === "completed") {
    return { status: "ready" }
  }
  if (status === "failed" || status === "error") {
    return { status: "failed", error: data.error || "Unknown error" }
  }
  return { status: "running" }
}

// Fetch snapshot data
async function fetchSnapshot(snapshotId: string, apiKey: string) {
  const progress = await checkProgress(snapshotId, apiKey)
  if (progress.status === "running") {
    return { status: "running" }
  }
  if (progress.status === "failed") {
    throw new Error(`Scrape failed: ${progress.error}`)
  }

  const response = await fetch(
    `${BRIGHTDATA_API_BASE}/snapshot/${snapshotId}?format=json`,
    { headers: { Authorization: `Bearer ${apiKey}` } }
  )

  if (response.status === 202) {
    return { status: "running" }
  }

  if (!response.ok) {
    throw new Error(`Snapshot fetch failed: ${response.status}`)
  }

  const data = await response.json()
  return { status: "ready", data }
}

// The task
export const fetchLinkedInJobs = schemaTask({
  id: "fetch-linkedin-jobs",
  schema: InputSchema,
  retry: {
    maxAttempts: 3,
    minTimeoutInMs: 2000,
    maxTimeoutInMs: 30000,
    factor: 2,
  },
  maxDuration: 600, // 10 min max

  run: async (payload) => {
    const apiKey = process.env.BRIGHTDATA_API_KEY
    if (!apiKey) {
      throw new Error("BRIGHTDATA_API_KEY not configured")
    }

    logger.info("Triggering LinkedIn jobs scrape", { input: payload })

    // Step 1: Trigger
    const snapshotId = await triggerScrape(payload, apiKey)
    logger.info(`Snapshot created: ${snapshotId}`)

    // Step 2: Poll with progressive intervals
    let attempts = 0
    let jobs = []

    while (attempts < MAX_POLL_ATTEMPTS) {
      attempts++
      const interval = getPollInterval(attempts)
      logger.info(`Polling (attempt ${attempts}/${MAX_POLL_ATTEMPTS}, next check in ${interval}s)`)

      const result = await fetchSnapshot(snapshotId, apiKey)

      if (result.status === "ready" && result.data) {
        jobs = result.data
        logger.info(`Received ${jobs.length} jobs after ${attempts} attempts`)
        break
      }

      await wait.for({ seconds: interval })
    }

    if (jobs.length === 0) {
      return {
        success: false,
        jobs: [],
        error: "Scraping timed out or returned no data",
        snapshotId,
      }
    }

    return {
      success: true,
      jobs,
      error: null,
      snapshotId,
    }
  },
})
```

### Example 2: API Route for Triggering Job Fetch

```typescript
// apps/web/src/app/api/v1/jobs/trigger-fetch/route.ts
import { NextRequest, NextResponse } from "next/server"
import { verifyAuth } from "@/lib/auth.server"
import { tasks } from "@trigger.dev/sdk/v3"

export async function POST(req: NextRequest) {
  // Verify auth
  const auth = await verifyAuth(req)
  if (!auth.authenticated || !auth.team_id) {
    return NextResponse.json({ error: "Unauthorized" }, { status: 401 })
  }

  // Parse body
  const body = await req.json()
  const { keyword, location, country, timeRange } = body

  // Validate input
  if (!keyword || !location || !country) {
    return NextResponse.json(
      { error: "Missing required fields: keyword, location, country" },
      { status: 400 }
    )
  }

  // Trigger task
  const handle = await tasks.trigger("fetch-linkedin-jobs", {
    keyword,
    location,
    country,
    timeRange: timeRange || "Past week",
  })

  // Store run ID in database (job_fetch_runs table)
  // ... (omitted for brevity)

  return NextResponse.json({
    runId: handle.id,
    status: "triggered",
  })
}
```

---

## Sources

- [BrightData Datasets API Overview](https://docs.brightdata.com/datasets/scrapers/scrapers-library/overview)
- [Trigger Asynchronous Data Collection](https://docs.brightdata.com/api-reference/web-scraper-api/asynchronous-requests)
- [Get Snapshots API](https://docs.brightdata.com/api-reference/web-scraper-api/management-apis/get-snapshots)
- [Snapshot Data Retrieval](https://docs.brightdata.com/api-reference/web-scraper-api/management-apis/snapshot-data)
- [Monitor Progress API](https://docs.brightdata.com/api-reference/web-scraper-api/management-apis/monitor-progress)
- [Deliver Snapshot](https://docs.brightdata.com/api-reference/marketplace-dataset-api/deliver-snapshot)
- [Custom Dataset API](https://docs.brightdata.com/datasets/custom-datasets/custom-dataset-api)
- [BrightData Pricing](https://www.firecrawl.dev/blog/bright-data-pricing)
- [Dataset Marketplace Pricing](https://brightdata.com/pricing/datasets)
- [LinkedIn Jobs Scraper](https://brightdata.com/products/web-scraper/linkedin/jobs)
- [Job Scraping n8n Template](https://n8n.io/workflows/6371-job-scraping-using-linkedin-indeed-bright-data-google-sheets/)
- [Upwork Scraper](https://brightdata.com/products/web-scraper/upwork)
- [BrightData Error Handling](https://docs.brightdata.com/api-reference/marketplace-dataset-api/troubleshooting)
- [Webhook Delivery](https://docs.brightdata.com/scraping-automation/serp-api/asynchronous-requests)
- [n8n Webhook Template](https://n8n.io/workflows/3866-asynchronous-bulk-web-scraping-with-bright-data-and-webhook-notifications/)

---

**End of Research Document**
*For questions or updates, contact Mark or review BrightData official docs at https://docs.brightdata.com*
